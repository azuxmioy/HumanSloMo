h5_file: 'HumanSlomo.h5'         # Change to your directory 
model_pretrain_G: 'checkpoints/netG_epoch006.pth'
model_pretrain_D: 'checkpoints/netD_epoch006.pth'
optimizer: ''

##############################
#    Optimizer parameters    # 
##############################
nr_epochs: 200
lr_policy: step                  # learning rate scheduler [multistep|constant|step]
beta1: 0.0                       # Adam hyperparameter
beta2: 0.999                     # Adam hyperparameter
gamma: 0.5                       # Learning Rate Decay
weight_decay: 0.0005             # weight decay
step_size: 20                    # epochs to decay the learning rate by gamma
lr: 0.0001                       # initial model learning rate
lr_d: 0.0004                     # initial discriminator learning rate

##################################
#    Visualization parameters    # 
##################################

display_freq: 20                 # number of steps to write losses to tensorboard
print_freq: 200                  # number of steps to print losses to command line

eval_step: 4                     # number of epochs to evaluate on validation set
save_step: 4                     # number of epochs to save the model
num_image: 1                     # number of images to write to tensorboard
update_frame_step: 10            # number of epochs to increase the number of frames in the training sample

##############################
#     Pose generator        # 
##############################

gen:                             # network parameters for human image generator
    num_frames_G: 2
    input_image_nc: 3
    input_label_nc: 22
    num_filters: 16
    max_num_filters: 512
    num_layers: 6
    num_downsamples: 4
    kernel_size: 3
    activation_norm_type: spatially_adaptive
    activation_norm_params:
        activation_norm_type: instance
        num_filters: 0
        kernel_size: 1
    weight_norm_type: spectral
    do_checkpoint: True
    mask:                        # network parameters for the mask prediction model
        generate_raw_output: False
        num_filters: 32
        max_num_filters: 512
        num_downsamples: 3
        num_res_blocks: 4
        kernel_size: 3
        activation_norm_type: instance
        weight_norm_type: spectral
    embed:                       # network parameters for the conditional encoder
        use_embed: True
        arch: encoder
        num_filters: 64
        max_num_filters: 512
        num_downsamples: 4
        kernel_size: 3
        weight_norm_type: spectral

##############################
#     Image Discriminator    # 
##############################
dis:                           # network parameters for the discriminator
    input_image_nc: 3
    input_label_nc: 22
    num_frames_D: 2
    few_shot: False
    type: imaginaire.discriminators.fs_vid2vid
    image:                     
        num_filters: 32
        max_num_filters: 512
        num_discriminators: 2
        num_layers: 4
        weight_norm_type: spectral
        activation_norm_type: instance
    additional_discriminators:
        face:                 # discriminator for the cropped face image
            num_filters: 32
            max_num_filters: 512
            num_discriminators: 1
            num_layers: 3
            weight_norm_type: spectral
            activation_norm_type: instance
            loss_weight: 10.0
            crop_func: utils.utils::crop_face_from_output
            vis: imaginaire.model_utils.fs_vid2vid::get_face_bbox_for_output
        hand:                 # discriminator for the cropped hand image
           num_filters: 32
           max_num_filters: 512
           num_discriminators: 1
           num_layers: 3
           weight_norm_type: spectral
           activation_norm_type: instance
           loss_weight: 10.0
           crop_func: utils.utils::crop_hand_from_output
           vis: imaginaire.model_utils.fs_vid2vid::get_hand_bbox_for_output


##############################
#   Loss function param s    # 
##############################
gan_mode: hinge

# weight for using GAN loss
gan:
    fuse: 0.0
    raw: 1.0
    face: 0.1
    hand: 0.1

# weight for GAN feature matching loss
fm_w: 1.0

# weight for using peerceptual loss
perceptual:
    weight: 10.0
    model: vgg19
    layers:
      - 'relu_1_1'
      - 'relu_2_1'
      - 'relu_3_1'
      - 'relu_4_1'
      - 'relu_5_1'
    weights:
      - 0.03125
      - 0.0625
      - 0.125
      - 0.25
      - 1.0
    criterion: 'l1'
    num_scales: 1

# weight for L1 reconstruction loss
l1_w: 30.0

# weight for mask regularization
mask_w: 5.0

###################################
#    Evaluation setting    # 
###################################
test_video_list : ['test_001', 'test_006', 'test_011', 'test_016', 'test_021', 'test_026']
gen_videos: true
eval_frames: 40    # max number of frames to use in the validation

###################################
#    Dataset training setting    # 
###################################
train_video_list: [
                   '00_Dance_001', '00_Dance_002', '00_Dance_003', '00_Dance_004', '00_Dance_005',
                   '01_Dance_011', '01_Dance_012', '01_Dance_013', '01_Dance_014', '01_Dance_015',
                   '02_Boxing_104', '02_Boxing_104', '02_Boxing_104', '02_Boxing_104',
                   '03_Boxing_111', '03_Boxing_112', '03_Boxing_113', '03_Boxing_114', '03_Boxing_115', '03_Boxing_116',
                   '03_Boxing_117', '03_Boxing_119', '03_Boxing_120', '03_Boxing_121', '03_Boxing_122',
                   '04_Basketball_201', '04_Basketball_202', '04_Basketball_203', '04_Basketball_204', '04_Basketball_205',
                   '04_Basketball_206', '04_Basketball_207', '04_Basketball_208', '04_Basketball_209', '04_Basketball_210',
                   '04_Basketball_211', '04_Basketball_212', '04_Basketball_213', 
                   '05_Body_301', '05_Body_302', '05_Body_303', '05_Body_304', '05_Body_305',
                   '05_Body_306', '05_Body_307', '05_Body_308', '05_Body_309', '05_Body_310',
                   '06_Body_311', '06_Body_312', '06_Body_313', '06_Body_314', '06_Body_315',
                   '06_Body_316', '06_Body_317', '06_Body_318', '06_Body_319', '06_Body_320',
                   '07_Body_321', '07_Body_322', '07_Body_323', '07_Body_324', '07_Body_325', 
                   '08_Kungfu_401', '08_Kungfu_402', '08_Kungfu_403', '08_Kungfu_404', '08_Kungfu_405',
                   '08_Kungfu_406', '08_Kungfu_407', '08_Kungfu_408', '08_Kungfu_409', '08_Kungfu_410',
                   '08_Kungfu_411', '08_Kungfu_412', '08_Kungfu_413', '08_Kungfu_414', '08_Kungfu_415',
                   '09_Kungfu_421', '09_Kungfu_422', '09_Kungfu_423', '09_Kungfu_424', '09_Kungfu_425', '09_Kungfu_426',
                   '09_Kungfu_427', '09_Kungfu_428', '09_Kungfu_429', '09_Kungfu_430', '09_Kungfu_431',
                   '10_Kungfu_441', '10_Kungfu_442', 
                   '11_Kungfu_451'
                    ]


max_frames: 4              # max number of frames to use in the training samples
random_drop_prob: 0.02     # probablistic to randomly drop a body joint
random_blur_rate: 0.06     # probablistic to randomly blur a body parts in the background image
gauss_sigma: 5             # bluring parameters
skeleton_thres: 0.001      # confidence threshold to use the body joints
foot_thres: 0.001          # confidence threshold to use the foot joints

load_width: 480            # load the image with the width and height for preprocessing 
load_height: 320           #

model_width: 480           # image size to feed into a model, if model size is smaller than load size
model_height: 320          # we will perform random cropping on the image to downscale to model size

